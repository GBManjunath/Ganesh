{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3ZbwzRFAWGKBxFNsImlLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GBManjunath/Ganesh/blob/main/Untitled62.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Present an overview of the AlexNet architecture\n",
        "AlexNet, developed by Alex Krizhevsky et al. in 2012, is a deep convolutional neural network that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). It consists of 8 layers:\n",
        "\n",
        "Input Layer: A 224x224x3 image (RGB).\n",
        "Convolutional Layers (Conv1, Conv2, Conv3, Conv4, Conv5): These layers extract features from the input image. They have increasing depth and decreasing spatial resolution as you move deeper into the network.\n",
        "Max Pooling Layers: These layers reduce the spatial size of the feature maps, making the model more computationally efficient.\n",
        "Fully Connected Layers: These layers perform classification based on the extracted features.\n",
        "Output Layer: A softmax layer with 1000 units, representing the 1000 classes in the ImageNet dataset.\n",
        "Q2: Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance\n",
        "Deep Architecture: AlexNet was much deeper than previous CNN architectures, with 8 layers (5 convolutional and 3 fully connected).\n",
        "ReLU Activation: Unlike the sigmoid or tanh activation functions used previously, AlexNet used the ReLU (Rectified Linear Unit) activation, which helped accelerate training by mitigating the vanishing gradient problem.\n",
        "Dropout: To prevent overfitting, AlexNet introduced dropout in the fully connected layers.\n",
        "Data Augmentation: The use of data augmentation (flipping, cropping, etc.) helped improve generalization by artificially increasing the size of the training dataset.\n",
        "GPU Utilization: AlexNet was one of the first models to leverage GPU parallelization for training, significantly speeding up the process.\n",
        "Q3: Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet\n",
        "Convolutional Layers: The convolutional layers in AlexNet are responsible for learning the spatial features of the input image. These layers apply filters that detect low-level patterns like edges and textures in the earlier layers and more complex patterns in deeper layers.\n",
        "\n",
        "Pooling Layers: Pooling layers (specifically max pooling) reduce the spatial dimensions of the feature maps, allowing the network to capture more abstract features and reduce the computational cost of training. These layers contribute to translation invariance.\n",
        "\n",
        "Fully Connected Layers: After feature extraction, the fully connected layers combine these features to make predictions. The output layer uses a softmax activation to classify the image into one of the 1000 ImageNet categories.\n",
        "\n",
        "Q4: Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice\n",
        "You can implement AlexNet using a framework like TensorFlow or PyTorch. Here's an example in TensorFlow:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define AlexNet architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPool2D((3, 3), strides=(2, 2)),\n",
        "    layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n",
        "    layers.MaxPool2D((3, 3), strides=(2, 2)),\n",
        "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
        "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
        "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPool2D((3, 3), strides=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(4096, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(4096, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1000, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# You can train on a dataset like ImageNet or CIFAR-10 (resize to 224x224)\n",
        "This script demonstrates the architecture of AlexNet in TensorFlow and sets up the model for training. The performance evaluation can be done by testing it on a suitable dataset (ImageNet or CIFAR-10)."
      ],
      "metadata": {
        "id": "o91Xllf7qFO3"
      }
    }
  ]
}