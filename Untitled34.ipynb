{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNweW+Ft9ayRUm0rXk5i6cI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GBManjunath/Ganesh/blob/main/Untitled34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF_oRFi9p0d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "This problem is an application of Bayes' theorem. We are asked to find the probability that an employee is a smoker given that they use the health insurance plan.\n",
        "\n",
        "Let:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "P(S∣H) be the probability that an employee is a smoker given that they use the health insurance plan.\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "P(H) be the probability that an employee uses the health insurance plan.\n",
        "�\n",
        "(\n",
        "�\n",
        "∩\n",
        "�\n",
        ")\n",
        "P(S∩H) be the probability that an employee is a smoker and uses the health insurance plan.\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "P(S) be the probability that an employee is a smoker.\n",
        "We are given the following information:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "0.7\n",
        "P(H)=0.7 (70% of employees use the health insurance plan).\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "=\n",
        "0.4\n",
        "P(S∣H)=0.4 (40% of employees who use the health insurance plan are smokers).\n",
        "From Bayes' theorem:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "=\n",
        "�\n",
        "(\n",
        "�\n",
        "∩\n",
        "�\n",
        ")\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "P(S∣H)=\n",
        "P(H)\n",
        "P(S∩H)\n",
        "​\n",
        "\n",
        "We already know that\n",
        "�\n",
        "(\n",
        "�\n",
        "∩\n",
        "�\n",
        ")\n",
        "=\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "⋅\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "P(S∩H)=P(S∣H)⋅P(H), so:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "=\n",
        "0.4\n",
        "×\n",
        "0.7\n",
        "0.7\n",
        "=\n",
        "0.4\n",
        "P(S∣H)=\n",
        "0.7\n",
        "0.4×0.7\n",
        "​\n",
        " =0.4\n",
        "Therefore, the probability that an employee is a smoker given that they use the health insurance plan is 0.4 or 40%.\n",
        "\n",
        "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of the Naive Bayes classifier used for classification tasks, but they differ in how they handle the input data, especially in the context of feature representations.\n",
        "\n",
        "Bernoulli Naive Bayes:\n",
        "\n",
        "Assumes binary features, meaning that each feature is either present or absent (0 or 1).\n",
        "The model is based on Bernoulli distribution, which is suitable for binary or boolean data.\n",
        "Example: If the data represents whether a word appears in an email, each feature would represent the presence or absence of that word.\n",
        "It is often used for problems like document classification when only the occurrence of a feature matters, not the number of times the feature occurs.\n",
        "Multinomial Naive Bayes:\n",
        "\n",
        "Assumes that the features are discrete counts of events or occurrences.\n",
        "The model is based on the Multinomial distribution, and it works well when features represent counts or frequencies of events.\n",
        "Example: If the data represents the frequency of each word in an email, each feature would represent the count of occurrences of that word.\n",
        "It is often used for tasks like text classification where the number of times a word appears in a document is important.\n",
        "Key Difference:\n",
        "\n",
        "Bernoulli Naive Bayes handles binary features (yes/no, present/absent), whereas Multinomial Naive Bayes handles count data (frequency of occurrences).\n",
        "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "Bernoulli Naive Bayes, by its nature, works with binary data, and handling missing values can be tricky in such cases. There are several approaches to deal with missing data in Bernoulli Naive Bayes:\n",
        "\n",
        "Ignoring the missing values: If a feature value is missing, it can be ignored during training and testing. However, this approach can lead to loss of valuable information.\n",
        "\n",
        "Imputation: Another approach is to impute missing values by assigning a default value (such as the most frequent class or a zero if the absence of a feature is most common). For example, if the feature represents whether a word is present in an email, we could impute missing values as \"not present.\"\n",
        "\n",
        "Laplace Smoothing: Bernoulli Naive Bayes often uses Laplace smoothing to handle zero occurrences of a feature. This can also help in situations where certain features have missing values.\n",
        "\n",
        "In practice, for missing values in Bernoulli Naive Bayes, either imputation or ignoring the missing values is commonly used, depending on the dataset and problem context.\n",
        "\n",
        "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "Yes, Gaussian Naive Bayes can be used for multi-class classification. In fact, Naive Bayes classifiers, including Gaussian Naive Bayes, are naturally capable of handling multi-class classification problems.\n",
        "\n",
        "Gaussian Naive Bayes works by assuming that the features are normally distributed (Gaussian distribution) for each class.\n",
        "For multi-class classification, Gaussian Naive Bayes will calculate the posterior probabilities for each class based on the features' distributions, and the class with the highest probability will be selected as the predicted class.\n",
        "Therefore, Gaussian Naive Bayes can easily handle more than two classes, and it is commonly used in multi-class problems, such as classifying data into multiple categories.\n",
        "\n",
        "Q5. Assignment: Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python.\n",
        "Here is a step-by-step guide to implement and evaluate the three types of Naive Bayes classifiers using scikit-learn on the Spambase dataset:\n",
        "\n",
        "Data Preparation:\n",
        "\n",
        "Download the Spambase dataset from the UCI Machine Learning Repository:\n",
        "\n",
        "Spambase Data Set\n",
        "\n",
        "Load and Preprocess the Data:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/spambase.data\"\n",
        "column_names = [f\"f{i}\" for i in range(57)] + [\"class\"]\n",
        "data = pd.read_csv(url, header=None, names=column_names)\n",
        "\n",
        "# Split dataset into features and target\n",
        "X = data.drop(columns=[\"class\"])\n",
        "y = data[\"class\"]\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data for GaussianNB\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "Train and Evaluate Models:\n",
        "\n",
        "Bernoulli Naive Bayes:\n",
        "python\n",
        "Copy code\n",
        "bnb = BernoulliNB()\n",
        "bnb_scores = cross_val_score(bnb, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
        "Multinomial Naive Bayes:\n",
        "python\n",
        "Copy code\n",
        "mnb = MultinomialNB()\n",
        "mnb_scores = cross_val_score(mnb, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
        "Gaussian Naive Bayes:\n",
        "python\n",
        "Copy code\n",
        "gnb = GaussianNB()\n",
        "gnb_scores = cross_val_score(gnb, X_train_scaled, y_train, cv=10, scoring=\"accuracy\")\n",
        "Metrics Calculation:\n",
        "\n",
        "After training each model, we evaluate the accuracy, precision, recall, and F1-score on the test set:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Evaluate on test set\n",
        "bnb.fit(X_train, y_train)\n",
        "y_pred_bnb = bnb.predict(X_test)\n",
        "print(\"Bernoulli Naive Bayes Performance\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bnb))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_bnb))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_bnb))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_bnb))\n",
        "Repeat the above evaluation for Multinomial Naive Bayes and Gaussian Naive Bayes.\n",
        "\n",
        "Discussion:\n",
        "\n",
        "After evaluating the models using cross-validation and on the test set, we will compare the performance based on:\n",
        "\n",
        "Accuracy: How often the classifier makes correct predictions.\n",
        "Precision: How many selected items are relevant (true positives).\n",
        "Recall: How many relevant items are selected.\n",
        "F1-Score: Harmonic mean of precision and recall.\n",
        "Conclusion:\n",
        "\n",
        "Summarize your findings:\n",
        "\n",
        "Which classifier performed the best and why.\n",
        "Discuss the advantages and disadvantages of each Naive Bayes variant.\n",
        "Identify any limitations of the Naive Bayes algorithm in this case (e.g., assumption of feature independence, sensitivity to outliers, etc.)."
      ],
      "metadata": {
        "id": "2ZAqW9pNuTRU"
      }
    }
  ]
}