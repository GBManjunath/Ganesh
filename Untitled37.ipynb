{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpFA9TMCc/uDytwS14nhXi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GBManjunath/Ganesh/blob/main/Untitled37.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF_oRFi9p0d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q1. Designing a Machine Learning Pipeline with Feature Engineering and Handling Missing Values\n",
        "To design a pipeline for the task described, we can use Scikit-learn's powerful tools like ColumnTransformer, Pipeline, and transformers for imputation, scaling, and encoding. Here's a step-by-step solution, including code snippets and explanations for each step.\n",
        "\n",
        "1. Automated Feature Selection\n",
        "First, we'll use an automated feature selection method to identify important features. In this case, SelectKBest with a statistical test can be used. For numerical features, f_classif is suitable, and for categorical features, we can use chi2.\n",
        "\n",
        "2. Numerical Pipeline (Imputation and Standardization)\n",
        "We'll first impute the missing values in numerical columns using the mean of the column values. Then, we'll scale the numerical columns using standardization.\n",
        "\n",
        "3. Categorical Pipeline (Imputation and One-Hot Encoding)\n",
        "For categorical features, we'll impute the missing values using the most frequent value and then apply one-hot encoding.\n",
        "\n",
        "4. Combine the Pipelines with ColumnTransformer\n",
        "We'll combine the numerical and categorical transformations using ColumnTransformer.\n",
        "\n",
        "5. Model Building with Random Forest Classifier\n",
        "Finally, we'll use the Random Forest Classifier to build the model.\n",
        "\n",
        "6. Evaluate the Model\n",
        "We'll evaluate the model using accuracy on the test dataset.\n",
        "\n",
        "Step-by-Step Code for the Pipeline\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Dataset\n",
        "# For demonstration purposes, using a placeholder dataset.\n",
        "# Replace with the actual dataset loading.\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# 2. Separate Features and Target Variable\n",
        "X = data.drop('target', axis=1)  # Replace 'target' with actual target column name\n",
        "y = data['target']\n",
        "\n",
        "# 3. Split Data into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 4. Define Numerical and Categorical Columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# 5. Create Numerical Pipeline\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),      # Impute missing values with the mean\n",
        "    ('scaler', StandardScaler())                      # Standardize numerical values\n",
        "])\n",
        "\n",
        "# 6. Create Categorical Pipeline\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))      # One-Hot Encoding for categorical variables\n",
        "])\n",
        "\n",
        "# 7. Combine the Numerical and Categorical Pipelines\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numerical_pipeline, numerical_cols),\n",
        "    ('cat', categorical_pipeline, categorical_cols)\n",
        "])\n",
        "\n",
        "# 8. Create Full Pipeline with Feature Selection and Random Forest Classifier\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selection', SelectKBest(f_classif, k='all')),  # Feature selection (use 'k=all' for all features)\n",
        "    ('classifier', RandomForestClassifier(random_state=42))   # Random Forest Classifier\n",
        "])\n",
        "\n",
        "# 9. Train the Model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# 10. Make Predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# 11. Evaluate the Model's Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the model: {accuracy:.4f}\")\n",
        "\n",
        "# 12. Interpretation and Possible Improvements\n",
        "# Interpretation: The accuracy score provides insight into how well the model is generalizing to unseen data.\n",
        "# Improvements:\n",
        "# - Try different feature selection techniques (e.g., Recursive Feature Elimination).\n",
        "# - Experiment with different classifiers (e.g., Gradient Boosting, SVM).\n",
        "# - Optimize the Random Forest parameters (e.g., n_estimators, max_depth).\n",
        "Explanation of Each Step:\n",
        "Data Loading: Load the dataset and split into features (X) and target (y).\n",
        "Imputation & Scaling: For numerical features, missing values are imputed with the mean and scaled using standardization. For categorical features, missing values are imputed with the most frequent value and then one-hot encoded.\n",
        "Feature Selection: We use SelectKBest with the f_classif function to perform feature selection. This method selects features based on their statistical relationship with the target variable.\n",
        "Modeling: The Random Forest Classifier is trained on the processed data.\n",
        "Evaluation: The accuracy score on the test set evaluates the model's performance.\n",
        "Q2. Build a Voting Classifier with Random Forest and Logistic Regression on the Iris Dataset\n",
        "Now, we'll implement a pipeline with a Voting Classifier that combines predictions from Random Forest and Logistic Regression on the Iris dataset.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_clf = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create a Voting Classifier\n",
        "voting_clf = VotingClassifier(estimators=[('rf', rf_clf), ('lr', lr_clf)], voting='hard')\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('voting', voting_clf)\n",
        "])\n",
        "\n",
        "# Train the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of the Voting Classifier: {accuracy:.4f}\")\n",
        "Explanation of Each Step:\n",
        "Data Loading: The Iris dataset is loaded using load_iris() from Scikit-learn.\n",
        "Base Classifiers: We create two classifiers: a Random Forest Classifier and a Logistic Regression Classifier.\n",
        "Voting Classifier: The VotingClassifier combines the base classifiers. We use hard voting, meaning that each base model votes for a class, and the class with the most votes is selected.\n",
        "Pipeline: The Voting Classifier is integrated into a pipeline for consistency in preprocessing and model training.\n",
        "Training & Evaluation: The pipeline is trained on the training set and evaluated on the test set.\n",
        "Interpretation:\n",
        "Accuracy: The accuracy score measures how well the combined classifiers (Random Forest and Logistic Regression) perform when combined in a voting mechanism. The performance will likely be better than using either classifier alone since the ensemble leverages the strengths of both models.\n",
        "Possible Improvements:\n",
        "Voting Mechanism: You can experiment with soft voting where the predicted class is based on the probabilities rather than majority voting.\n",
        "Hyperparameter Tuning: Optimize the parameters of both classifiers to improve performance (e.g., n_estimators in Random Forest, C in Logistic Regression)"
      ],
      "metadata": {
        "id": "HOFVLDtIv2a3"
      }
    }
  ]
}