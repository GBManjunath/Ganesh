{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPXenHz6BSIqWqr8dg9mXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GBManjunath/Ganesh/blob/main/Untitled63.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Core Components of the Hadoop Ecosystem\n",
        "The Hadoop ecosystem is a set of tools and frameworks that work together to process and store large datasets across distributed computing environments. The core components are:\n",
        "\n",
        "HDFS (Hadoop Distributed File System): It is a distributed file system designed to store large amounts of data across multiple machines. HDFS splits large files into blocks (typically 128 MB or 256 MB) and stores these blocks across various machines to ensure fault tolerance and scalability.\n",
        "\n",
        "MapReduce: This is a programming model for processing large datasets. It divides tasks into two phases: Map and Reduce. In the Map phase, data is split into smaller chunks and processed in parallel. In the Reduce phase, the results from the Map phase are aggregated. MapReduce allows parallel processing, which improves performance and scalability.\n",
        "\n",
        "YARN (Yet Another Resource Negotiator): YARN is the resource management layer in Hadoop that manages resources in the cluster. It allocates resources to applications, handles job scheduling, and monitors resource usage. YARN replaced the JobTracker and TaskTracker in the older Hadoop 1.x architecture, providing more scalability and efficiency.\n",
        "\n",
        "2. Hadoop Distributed File System (HDFS)\n",
        "HDFS is designed for reliable and efficient storage of large datasets across multiple machines in a cluster. It breaks data into fixed-size blocks and stores each block in multiple copies (replicas) across different nodes to ensure fault tolerance.\n",
        "\n",
        "Key Components of HDFS:\n",
        "\n",
        "NameNode: It is the master server that manages the metadata and structure of the file system. The NameNode keeps track of the blocks and their locations in the cluster but does not store the actual data.\n",
        "DataNode: These are worker nodes that store the actual data blocks. Each DataNode regularly reports to the NameNode about the blocks it stores.\n",
        "Blocks: Data in HDFS is divided into blocks (typically 128 MB). Blocks are distributed across the cluster and replicated to ensure data reliability and availability.\n",
        "Fault Tolerance: HDFS replicates each block (by default 3 copies) to multiple DataNodes. If one DataNode fails, another replica can be used, ensuring no data loss.\n",
        "3. MapReduce Framework\n",
        "MapReduce is a programming model and a processing framework for handling large-scale data. It works by dividing a job into two phases:\n",
        "\n",
        "Map Phase: Input data is divided into key-value pairs, which are processed by mapper functions in parallel. For example, in a word-count program, the text data is broken into words, and each word is emitted as a key-value pair (word, 1).\n",
        "\n",
        "Reduce Phase: The key-value pairs from the Map phase are grouped by key and passed to the Reducer, which aggregates the data. For example, the Reducer sums up the counts for each word.\n",
        "\n",
        "Real-world Example: In a word-count application:\n",
        "\n",
        "Map phase: Split the text into words and emit (word, 1).\n",
        "Reduce phase: Aggregate the counts by summing up the values for each word.\n",
        "Advantages of MapReduce:\n",
        "\n",
        "Scalable and parallelizes computation.\n",
        "Fault-tolerant: If a node fails, the task is reassigned to another node.\n",
        "Suitable for batch processing of large datasets.\n",
        "Limitations:\n",
        "\n",
        "Not suitable for iterative algorithms (e.g., machine learning, graph processing) as each MapReduce job is independent.\n",
        "It has a higher latency for small jobs.\n",
        "4. Role of YARN in Hadoop\n",
        "YARN (Yet Another Resource Negotiator) is the resource management layer introduced in Hadoop 2.x. It manages resources and schedules tasks in the cluster.\n",
        "\n",
        "Role:\n",
        "\n",
        "Resource Manager: It allocates resources (memory, CPU) to the various applications in the cluster.\n",
        "Node Manager: It manages resources on each individual node and reports resource usage to the Resource Manager.\n",
        "Application Master: It manages the lifecycle of a job, requesting resources from the Resource Manager and tracking its progress.\n",
        "Comparison with Hadoop 1.x: In Hadoop 1.x, the JobTracker was responsible for both job scheduling and resource management, which caused scalability issues. YARN decouples resource management and job scheduling, allowing for better scalability and resource utilization.\n",
        "\n",
        "5. Popular Components in the Hadoop Ecosystem\n",
        "HBase: A distributed NoSQL database that runs on top of HDFS. It is designed for low-latency random access to large datasets.\n",
        "\n",
        "Hive: A data warehouse infrastructure that provides a SQL-like query language (HiveQL) for querying and managing large datasets stored in HDFS.\n",
        "\n",
        "Pig: A high-level platform for creating MapReduce programs. It uses a language called Pig Latin for processing and analyzing large datasets.\n",
        "\n",
        "Spark: A fast, in-memory data processing engine that is much faster than MapReduce due to its ability to store intermediate data in memory instead of writing it to disk.\n",
        "\n",
        "Integration Example - Hive: Hive can be used in a Hadoop ecosystem to query large datasets stored in HDFS. It abstracts the complexities of MapReduce programming, providing a SQL-like interface to run queries.\n",
        "\n",
        "6. Differences Between Apache Spark and Hadoop MapReduce\n",
        "Speed: Spark performs in-memory processing, which makes it significantly faster than Hadoop MapReduce, which writes intermediate results to disk.\n",
        "Ease of Use: Spark provides high-level APIs in Java, Scala, Python, and R, while Hadoop MapReduce requires writing custom code for both the Map and Reduce phases.\n",
        "Support for Iterative Algorithms: Spark is ideal for iterative algorithms (such as machine learning) because it can cache data in memory between iterations, while MapReduce requires writing intermediate results to disk.\n",
        "7. Spark Application to Count Word Occurrences (in Python)\n",
        "python\n",
        "Copy code\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"WordCount\")\n",
        "\n",
        "# Load a text file\n",
        "text_file = sc.textFile(\"file:///path/to/textfile.txt\")\n",
        "\n",
        "# Count occurrences of each word\n",
        "word_counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "                       .map(lambda word: (word, 1)) \\\n",
        "                       .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Get the top 10 most frequent words\n",
        "top_words = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
        "\n",
        "# Print the result\n",
        "for word, count in top_words:\n",
        "    print(f\"{word}: {count}\")\n",
        "8. Spark RDD Operations\n",
        "python\n",
        "Copy code\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"RDD Operations\")\n",
        "\n",
        "# Load dataset\n",
        "data = sc.textFile(\"data.txt\")\n",
        "\n",
        "# a. Filter data\n",
        "filtered_data = data.filter(lambda line: \"specific_criteria\" in line)\n",
        "\n",
        "# b. Map transformation\n",
        "mapped_data = filtered_data.map(lambda line: line.upper())\n",
        "\n",
        "# c. Reduce operation to calculate sum\n",
        "reduced_data = mapped_data.map(lambda line: len(line)).reduce(lambda a, b: a + b)\n",
        "\n",
        "print(reduced_data)\n",
        "9. Spark DataFrame Operations\n",
        "python\n",
        "Copy code\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DataFrame Operations\").getOrCreate()\n",
        "\n",
        "# Load dataset\n",
        "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# a. Select columns\n",
        "df_selected = df.select(\"column1\", \"column2\")\n",
        "\n",
        "# b. Filter rows\n",
        "df_filtered = df_selected.filter(df[\"column1\"] > 50)\n",
        "\n",
        "# c. Group by and aggregate\n",
        "df_grouped = df_filtered.groupBy(\"column1\").agg({\"column2\": \"avg\"})\n",
        "\n",
        "# d. Join DataFrames\n",
        "df2 = spark.read.csv(\"data2.csv\", header=True, inferSchema=True)\n",
        "df_joined = df_filtered.join(df2, df_filtered[\"column1\"] == df2[\"column1\"])\n",
        "\n",
        "df_joined.show()\n",
        "10. Spark Streaming Application\n",
        "python\n",
        "Copy code\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext and StreamingContext\n",
        "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "# Create DStream to read from Kafka or simulated data source\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Transform data (e.g., word count)\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Output the processed data\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n",
        "11. Fundamental Concepts of Apache Kafka\n",
        "Apache Kafka is a distributed streaming platform designed to handle real-time data feeds. It allows you to:\n",
        "\n",
        "Publish data (Producers).\n",
        "Subscribe to streams of records (Consumers).\n",
        "Store streams of records in a fault-tolerant manner (Kafka Brokers).\n",
        "Process streams of records (Kafka Streams).\n",
        "Kafka is used to address issues such as real-time data integration, fault tolerance, and high throughput in big data systems.\n",
        "\n",
        "12. Architecture of Kafka\n",
        "Producer: Sends data to Kafka topics.\n",
        "Broker: Manages message storage and serves as a mediator between Producers and Consumers.\n",
        "Topic: A category to which messages are published.\n",
        "Consumer: Reads messages from topics.\n",
        "ZooKeeper: Manages Kafkaâ€™s metadata and coordinates distributed brokers.\n",
        "Kafka ensures high throughput, scalability, and durability, making it a popular choice for real-time data pipelines.\n",
        "\n",
        "13. Guide to Producing and Consuming Data in Kafka\n",
        "Producer in Python (using kafka-python library):\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from kafka import KafkaProducer\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
        "producer.send('my_topic', b'Hello, Kafka!')\n",
        "producer.flush()\n",
        "Consumer in Python:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "consumer = KafkaConsumer('my_topic', bootstrap_servers='localhost:9092')\n",
        "for message in consumer:\n",
        "    print(message.value)\n",
        "14. Data Retention and Partitioning in Kafka\n",
        "Kafka allows data retention policies to be configured by specifying a time-based retention period or size limits. Partitioning ensures parallel processing of data by splitting topics into smaller units (partitions) and distributing them across multiple brokers.\n",
        "\n",
        "15. Real-World Use Cases of Apache Kafka\n",
        "Kafka is used in scenarios where real-time data streaming and high throughput are required:\n",
        "\n",
        "Log Aggregation: Collecting logs from distributed systems and making them available for real-time monitoring.\n",
        "Real-time Analytics: Processing and analyzing data streams in real-time.\n",
        "Event Sourcing: Kafka can handle event-based architectures, providing a reliable way to track events across microservices."
      ],
      "metadata": {
        "id": "Bly5WguGrBxN"
      }
    }
  ]
}