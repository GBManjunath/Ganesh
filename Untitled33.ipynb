{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4kNvUypNYV6gttZYwX19t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GBManjunath/Ganesh/blob/main/Untitled33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF_oRFi9p0d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q1. What is Bayes' theorem?\n",
        "Bayes' theorem is a fundamental concept in probability theory and statistics that describes how to update the probability of a hypothesis (or event) based on new evidence or information. It allows us to calculate the probability of a hypothesis given some observed evidence, considering both the prior knowledge (prior probability) and the likelihood of the observed data under that hypothesis.\n",
        "\n",
        "In other words, Bayes' theorem provides a way to update our beliefs about the world (in terms of probabilities) as new data becomes available.\n",
        "\n",
        "Q2. What is the formula for Bayes' theorem?\n",
        "The formula for Bayes' theorem is:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "=\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "⋅\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "P(H∣E)=\n",
        "P(E)\n",
        "P(E∣H)⋅P(H)\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "P(H∣E) is the posterior probability: the probability of the hypothesis\n",
        "�\n",
        "H given the evidence\n",
        "�\n",
        "E.\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "P(E∣H) is the likelihood: the probability of observing the evidence\n",
        "�\n",
        "E, given the hypothesis\n",
        "�\n",
        "H.\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "P(H) is the prior probability: the initial probability of the hypothesis\n",
        "�\n",
        "H before observing the evidence\n",
        "�\n",
        "E.\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "P(E) is the evidence: the total probability of observing the evidence, which is the sum of the likelihood of the evidence under all possible hypotheses.\n",
        "Q3. How is Bayes' theorem used in practice?\n",
        "Bayes' theorem is widely used in various fields, including machine learning, medical diagnosis, finance, and data science, for updating beliefs or predicting outcomes based on new data. Some common applications include:\n",
        "\n",
        "Classification Problems:\n",
        "\n",
        "Naive Bayes Classifier: A machine learning algorithm that applies Bayes' theorem with the assumption that the features are conditionally independent given the class. It's used for classification tasks like spam filtering, sentiment analysis, and document classification.\n",
        "Medical Diagnosis:\n",
        "\n",
        "Bayes' theorem helps update the probability of a disease based on the results of diagnostic tests. For instance, if a test shows a positive result, Bayes' theorem can update the probability that a patient actually has the disease, considering the test's accuracy and the prevalence of the disease.\n",
        "Spam Filtering:\n",
        "\n",
        "In spam classification, Bayes' theorem is used to calculate the probability that an email is spam based on the words contained in the email. The classifier computes the probability of spam and non-spam given the observed words and their frequency.\n",
        "Search Engines:\n",
        "\n",
        "It can help rank web pages based on the probability that a page is relevant to a search query.\n",
        "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
        "Bayes' theorem is based on conditional probability. Conditional probability refers to the probability of an event occurring given that another event has already occurred. Bayes' theorem uses this concept to reverse the conditionality.\n",
        "\n",
        "In Bayes' theorem, we are interested in computing the posterior probability\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "P(H∣E), which is the probability of the hypothesis\n",
        "�\n",
        "H given the evidence\n",
        "�\n",
        "E. This is a conditional probability. To compute it, we use likelihood\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        ")\n",
        "P(E∣H), which is another form of conditional probability—this represents the probability of the evidence\n",
        "�\n",
        "E given the hypothesis\n",
        "�\n",
        "H.\n",
        "\n",
        "In summary, Bayes' theorem links the posterior probability with the prior and likelihood, and both prior and likelihood are conditional probabilities.\n",
        "\n",
        "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
        "In practice, there are three main types of Naive Bayes classifiers, and the choice of which one to use depends on the type of data and the distribution assumptions:\n",
        "\n",
        "Gaussian Naive Bayes:\n",
        "\n",
        "Use this classifier when the features are continuous and assumed to follow a Gaussian (normal) distribution. It estimates the mean and variance for each class and uses this to compute the likelihood.\n",
        "Use Case: When the features (e.g., height, weight) are continuous and approximately follow a normal distribution.\n",
        "Multinomial Naive Bayes:\n",
        "\n",
        "This classifier is used when the features are discrete and represent counts or frequencies (e.g., the number of times a word appears in a document). It assumes that the features follow a multinomial distribution.\n",
        "Use Case: Often used for text classification tasks, such as spam filtering or sentiment analysis, where features are word counts or term frequencies.\n",
        "Bernoulli Naive Bayes:\n",
        "\n",
        "This classifier is used when the features are binary (e.g., 0 or 1, indicating the presence or absence of a feature). It assumes that the features follow a Bernoulli distribution.\n",
        "Use Case: When you have binary features, such as in document classification tasks where each word is either present or absent.\n",
        "Q6. Assignment:\n",
        "Let's break down the problem. The task is to use Naive Bayes to classify a new instance with features\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        "X1=3 and\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        "X2=4, based on the following frequency table:\n",
        "\n",
        "Class\tX1=1\tX1=2\tX1=3\tX2=1\tX2=2\tX2=3\tX2=4\n",
        "A\t3\t3\t4\t4\t3\t3\t3\n",
        "B\t2\t2\t1\t2\t2\t2\t3\n",
        "Step 1: Calculate the prior probabilities for each class.\n",
        "Since we assume equal prior probabilities for each class, we can compute the prior probabilities as:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "Number of instances in class A or B\n",
        "Total number of instances\n",
        "=\n",
        "1\n",
        "2\n",
        "=\n",
        "0.5\n",
        "P(A)=P(B)=\n",
        "Total number of instances\n",
        "Number of instances in class A or B\n",
        "​\n",
        " =\n",
        "2\n",
        "1\n",
        "​\n",
        " =0.5\n",
        "Step 2: Calculate the likelihoods for each feature given the class.\n",
        "For Class A:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        "∣\n",
        "�\n",
        ")\n",
        "=\n",
        "Frequency of X1=3 for A\n",
        "Total count for A\n",
        "=\n",
        "4\n",
        "3\n",
        "+\n",
        "3\n",
        "+\n",
        "4\n",
        "=\n",
        "4\n",
        "10\n",
        "=\n",
        "0.4\n",
        "P(X1=3∣A)=\n",
        "Total count for A\n",
        "Frequency of X1=3 for A\n",
        "​\n",
        " =\n",
        "3+3+4\n",
        "4\n",
        "​\n",
        " =\n",
        "10\n",
        "4\n",
        "​\n",
        " =0.4\n",
        "�\n",
        "(\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        "∣\n",
        "�\n",
        ")\n",
        "=\n",
        "3\n",
        "3\n",
        "+\n",
        "3\n",
        "+\n",
        "3\n",
        "+\n",
        "3\n",
        "=\n",
        "3\n",
        "12\n",
        "=\n",
        "0.25\n",
        "P(X2=4∣A)=\n",
        "3+3+3+3\n",
        "3\n",
        "​\n",
        " =\n",
        "12\n",
        "3\n",
        "​\n",
        " =0.25\n",
        "For Class B:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        "∣\n",
        "�\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "+\n",
        "2\n",
        "+\n",
        "1\n",
        "=\n",
        "1\n",
        "5\n",
        "=\n",
        "0.2\n",
        "P(X1=3∣B)=\n",
        "2+2+1\n",
        "1\n",
        "​\n",
        " =\n",
        "5\n",
        "1\n",
        "​\n",
        " =0.2\n",
        "�\n",
        "(\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        "∣\n",
        "�\n",
        ")\n",
        "=\n",
        "3\n",
        "2\n",
        "+\n",
        "2\n",
        "+\n",
        "2\n",
        "+\n",
        "3\n",
        "=\n",
        "3\n",
        "9\n",
        "=\n",
        "0.333\n",
        "P(X2=4∣B)=\n",
        "2+2+2+3\n",
        "3\n",
        "​\n",
        " =\n",
        "9\n",
        "3\n",
        "​\n",
        " =0.333\n",
        "Step 3: Calculate the posterior probabilities for each class using Bayes' theorem.\n",
        "For Class A:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        ",\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        ")\n",
        "∝\n",
        "�\n",
        "(\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        "∣\n",
        "�\n",
        ")\n",
        "⋅\n",
        "�\n",
        "(\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        "∣\n",
        "�\n",
        ")\n",
        "⋅\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "0.4\n",
        "⋅\n",
        "0.25\n",
        "⋅\n",
        "0.5\n",
        "=\n",
        "0.05\n",
        "P(A∣X1=3,X2=4)∝P(X1=3∣A)⋅P(X2=4∣A)⋅P(A)=0.4⋅0.25⋅0.5=0.05\n",
        "For Class B:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        ",\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        ")\n",
        "∝\n",
        "�\n",
        "(\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        "∣\n",
        "�\n",
        ")\n",
        "⋅\n",
        "�\n",
        "(\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        "∣\n",
        "�\n",
        ")\n",
        "⋅\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "0.2\n",
        "⋅\n",
        "0.333\n",
        "⋅\n",
        "0.5\n",
        "=\n",
        "0.0333\n",
        "P(B∣X1=3,X2=4)∝P(X1=3∣B)⋅P(X2=4∣B)⋅P(B)=0.2⋅0.333⋅0.5=0.0333\n",
        "Step 4: Compare the posterior probabilities.\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        ",\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        ")\n",
        "=\n",
        "0.05\n",
        "P(A∣X1=3,X2=4)=0.05\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        ",\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        ")\n",
        "=\n",
        "0.0333\n",
        "P(B∣X1=3,X2=4)=0.0333\n",
        "Since\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        ",\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        ")\n",
        ">\n",
        "�\n",
        "(\n",
        "�\n",
        "∣\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        ",\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        ")\n",
        "P(A∣X1=3,X2=4)>P(B∣X1=3,X2=4), Naive Bayes will classify the new instance as Class A.\n",
        "\n",
        "Conclusion:\n",
        "Based on the Naive Bayes calculations, the new instance with\n",
        "�\n",
        "1\n",
        "=\n",
        "3\n",
        "X1=3 and\n",
        "�\n",
        "2\n",
        "=\n",
        "4\n",
        "X2=4 will be classified as Class A."
      ],
      "metadata": {
        "id": "ZFB3aGrxtiJa"
      }
    }
  ]
}